{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673df7f8-a9dd-4a61-911b-212121acf11a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criar um secret scope\n",
    "\n",
    "1º - Criar o secret no Key Vault da Azure\n",
    "\n",
    "2º - Acessar o endereço \"https://(databricks-instance-here)#secrets/createScope\" colocando a sua instacia do databricks\n",
    "\n",
    "3º - Preencher com as informações soicitadas\n",
    "  \n",
    "https://learn.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24b2990-cfa4-4641-ae4f-fb2c90fb8610",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Como definir variaveis de ambiente no cluster\n",
    "\n",
    "Na página de configuração do cluster, clique na alternância Opções Avançadas.\n",
    "\n",
    "1º - Clique na guia Spark.\n",
    "\n",
    "2º - Na configuração do Spark, insira as propriedades de configuração como um par chave-valor por linha, separados por espaço.\n",
    "\n",
    "https://learn.microsoft.com/pt-br/azure/databricks/compute/configure#spark-configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c7bde4-f246-4872-bf02-13219a3d18c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Acessar uma variavel de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbe100a-683e-4832-aad8-bb60c964e2e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_user = spark.conf.get(\"spark.sql_user\")\n",
    "sql_secret = spark.conf.get(\"spark.sql_secret\")\n",
    "server = spark.conf.get(\"server\")\n",
    "db = spark.conf.get(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a87e1e-b280-4de4-a1f9-de755376ce43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Montar um blob\n",
    "\n",
    "```python\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://<seu-container>@<seu-blob>.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/blob/dax\",\n",
    "  extra_configs = {\n",
    "    \"fs.azure.account.key.<seu-blob>.blob.core.windows.net\":\n",
    "    dbutils.secrets.get(\n",
    "      scope = \"secret-scope\", \n",
    "      key = \"key\"\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8e6457-1852-4de2-9215-0ee6503b417a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Bibliotecas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dacc56f-c866-4467-a93b-2239729ce838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8269f177-a0f1-4a26-9183-441a49a5b5d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Leitura e transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87afc88c-3b28-4de5-9365-2493b4f635b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"dbfs:/mnt/blob/dax*\")\n",
    "\n",
    "df_exp = df.select(\n",
    "    'data.symbol',\n",
    "    'data.ask',\n",
    "    'data.baseVolume24h',\n",
    "    'data.bid',\n",
    "    'data.high24h',\n",
    "    'data.lastPrice',\n",
    "    'data.low24h',\n",
    "    'data.open24h',\n",
    "    'data.quoteVolume24h',\n",
    "    'data.timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe0b8b4-3d61-445a-9ee5-1318770f1906",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_exp = (\n",
    "    df_exp\n",
    "    .withColumn(\"timestamp\", from_unixtime(df_exp.timestamp / 1000))\n",
    "    .withColumn(\"broker_id\", lit(\"1\"))\n",
    "    .withColumn(\"usuario_id\", lit(\"1\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b366d106-e943-4bfc-b740-8791e6c04f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_exp.select(\n",
    "    'broker_id',\n",
    "    'usuario_id',\n",
    "    'symbol',\n",
    "    'ask',\n",
    "    'baseVolume24h',\n",
    "    'bid',\n",
    "    'high24h',\n",
    "    'lastPrice',\n",
    "    'low24h',\n",
    "    'open24h',\n",
    "    'quoteVolume24h',\n",
    "    'timestamp'\n",
    ")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba40802b-90da-4c74-a2ef-6208b0086511",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Filtrar o DF usando a data max da tabela que esta no SQL, recebida como parametro do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8370f62d-3aa4-4536-9028-cfadb1953365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = dbutils.widgets.get(\"max_date\")\n",
    "df = df.filter(col('timestamp') > data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ce51d8-5471-40d6-8928-9c699afd1199",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Salvar no AzureSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a7168a-a97a-4331-a042-a1359ded74d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUrl = 'jdbc:sqlserver://{0}:{1};database={2}'.format(server, 1433, db)\n",
    "connectionProperties = {\n",
    "  'user' : sql_user,\n",
    "  'password' : sql_secret,\n",
    "  'driver' : 'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "}\n",
    "\n",
    "table = 'dax_api'\n",
    "\n",
    "(\n",
    "  df.write.mode('append')\n",
    "  .format('jdbc')\n",
    "  .option('url', jdbcUrl)\n",
    "  .option('dbtable', table)\n",
    "  .option('user', sql_user)\n",
    "  .option('password', sql_secret)\n",
    "  .option('driver', jdbcDriver)\n",
    "  .save()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NovaDaxAPI",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
